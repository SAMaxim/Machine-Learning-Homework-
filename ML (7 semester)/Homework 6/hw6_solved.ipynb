{"cells":[{"cell_type":"code","execution_count":null,"id":"45b70d8b","metadata":{"id":"45b70d8b"},"outputs":[],"source":["import math\n","import pickle\n","import random\n","from typing import List, Tuple\n","\n","import numpy as np\n","import torch\n","from catboost.datasets import msrank_10k\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.tree import DecisionTreeRegressor\n","from tqdm.auto import tqdm"]},{"cell_type":"code","execution_count":null,"id":"f8bd6a94","metadata":{"id":"f8bd6a94"},"outputs":[],"source":["def set_seed(seed: int) -> None:\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)"]},{"cell_type":"code","execution_count":null,"id":"a7d756a6","metadata":{"id":"a7d756a6"},"outputs":[],"source":["def dcg(ys_true: torch.FloatTensor, ys_pred: torch.FloatTensor, gain_scheme: str =\"exp2\", k: int = None) -> float:\n","    k = k or ys_true.size(dim=0)\n","    if k > ys_true.size(dim=0):\n","        k = ys_true.size(dim=0)\n","    idx = torch.argsort(ys_pred, descending=True)\n","    true_sorted = ys_true[idx].to(torch.float64)\n","    steps = torch.arange(2, k + 2, dtype=torch.float64)\n","    steps = torch.log2(steps)\n","    gains = true_sorted.apply_(lambda x: compute_gain(x, gain_scheme))[0:k]\n","    return float(torch.sum(gains / steps))\n","\n","def ndcg(ys_true: torch.FloatTensor, ys_pred: torch.FloatTensor, gain_scheme: str = 'exp2', k: int = None) -> float:\n","    k = k or ys_true.size(dim=0)\n","    if k > ys_true.size(dim=0):\n","        k =  ys_true.size(dim=0)\n","    dcg_score = dcg(ys_true, ys_pred, gain_scheme, k)\n","    true_sorted, _ = torch.sort(ys_true, descending=True)\n","    ideal_dcg = dcg(true_sorted, true_sorted, gain_scheme, k)\n","    return float(dcg_score / ideal_dcg)"]},{"cell_type":"code","execution_count":null,"id":"15f547d7","metadata":{"id":"15f547d7"},"outputs":[],"source":["def compute_gain(y_value: float, gain_scheme: str) -> float:\n","    if gain_scheme == 'const':\n","        return float(y_value)\n","    elif gain_scheme == 'exp2':\n","        return float(2 ** y_value - 1)\n","    return float(\"inf\")"]},{"cell_type":"code","execution_count":null,"id":"3630014a","metadata":{"id":"3630014a"},"outputs":[],"source":["class Solution:\n","    def __init__(self, n_estimators: int = 100, lr: float = 0.5, ndcg_top_k: int = 10,\n","                 subsample: float = 0.6, colsample_bytree: float = 0.9,\n","                 max_depth: int = 5, min_samples_leaf: int = 8):\n","        self.X_train = None\n","        self.ys_train = None\n","        self.X_test = None\n","        self.ys_test = None\n","        self._prepare_data()\n","\n","        self.ndcg_top_k = ndcg_top_k\n","        self.n_estimators = n_estimators  # количество деревьев\n","        self.lr = lr  # Learning Rate, коэффициент, на который умножаются предсказания каждого нового дерева\n","        self.max_depth = max_depth  # максимальная глубина\n","        self.min_samples_leaf = min_samples_leaf  # минимальное количество термальных листьев\n","\n","        self.subsample = subsample  # доля объектов от выборки\n","        self.colsample_bytree = colsample_bytree  # доля признаков от выборки\n","\n","        self.trees: List[DecisionTreeRegressor] = []  # все деревья\n","        self.idxs_array = []\n","        self.all_ndcg: List[float] = []\n","        self.best_ndcg = float(0.0)\n","\n","    def _get_data(self) -> List[np.ndarray]:\n","        train_df, test_df = msrank_10k()\n","\n","        X_train = train_df.drop([0, 1], axis=1).values\n","        y_train = train_df[0].values\n","        query_ids_train = train_df[1].values.astype(int)\n","\n","        X_test = test_df.drop([0, 1], axis=1).values\n","        y_test = test_df[0].values\n","        query_ids_test = test_df[1].values.astype(int)\n","\n","        return [X_train, y_train, query_ids_train, X_test, y_test, query_ids_test]\n","\n","    def _prepare_data(self) -> None:\n","        (X_train, y_train, self.query_ids_train,\n","         X_test, y_test, self.query_ids_test) = self._get_data()\n","        self.X_train = torch.FloatTensor(self._scale_features_in_query_groups(X_train, self.query_ids_train))\n","        self.ys_train = torch.FloatTensor(y_train)\n","        self.X_test = torch.FloatTensor(self._scale_features_in_query_groups(X_test, self.query_ids_test))\n","        self.ys_test = torch.FloatTensor(y_test)\n","        \n","    def _scale_features_in_query_groups(self, inp_feat_array: np.ndarray,\n","                                        inp_query_ids: np.ndarray) -> np.ndarray:\n","        for id in np.unique(inp_query_ids):\n","            scaler = StandardScaler()\n","            idxs = inp_query_ids == id\n","            inp_feat_array[idxs] = scaler.fit_transform(inp_feat_array[idxs])\n","        return inp_feat_array\n","\n","    def _train_one_tree(self, cur_tree_idx: int, train_preds: torch.FloatTensor) -> Tuple[DecisionTreeRegressor, np.ndarray]:\n","        \"\"\"\n","        Метод для тренировки одного дерева.\n","        @cur_tree_idx: номер текущего дерева, который предлагается использовать в качестве random_seed для того,\n","        чтобы алгоритм был детерминирован.\n","        @train_preds: суммарные предсказания всех предыдущих деревьев (для расчёта лямбд).\n","        @return: это само дерево и индексы признаков, на которых обучалось дерево\n","        \"\"\"\n","        # Устанавливаем seed для случайных операций\n","        set_seed(cur_tree_idx)\n","\n","        # Создаем словарь с индексами для каждого query_id\n","        query_indices = {}\n","        for i, query_id in enumerate(self.query_ids_train):\n","            if query_id not in query_indices:\n","                query_indices[query_id] = []\n","            query_indices[query_id].append(i)\n","\n","        # Создаем массив лямбда-значений для каждого примера в обучающей выборке\n","        lambdas = torch.zeros_like(train_preds)\n","\n","        # Рассчитываем лямбда-значения для каждого блока данных с одинаковым query_id\n","        for query_id, indices in query_indices.items():\n","            lambdas_query = self._compute_lambdas(self.ys_train[indices], train_preds[indices])\n","            lambdas[indices] = lambdas_query.squeeze()\n","\n","        # Случайным образом выбираем подмножества примеров и признаков для обучения дерева\n","        samples_count = self.X_train.size(dim=0)\n","        features_count = self.X_train.size(dim=1)\n","        samples_indices = torch.full((samples_count,), False)\n","        feature_indices = torch.full((features_count,), False)\n","        for i in range(samples_count):\n","            if np.random.rand() < self.subsample:\n","                samples_indices[i] = True\n","        for i in range(features_count):\n","            if np.random.rand() < self.colsample_bytree:\n","                feature_indices[i] = True\n","\n","        # Выбираем подмножество данных и обучаем дерево решений\n","        sub = self.X_train[samples_indices]\n","        sub = sub[:, feature_indices]\n","        lambdas_sub = lambdas[samples_indices]\n","        dtr = DecisionTreeRegressor(max_depth=self.max_depth, random_state=cur_tree_idx)\n","        dtr.fit(sub, lambdas_sub)\n","\n","        # Сохраняем дерево и индексы признаков\n","        return dtr, torch.where(feature_indices)[0]\n","\n","    def _calc_data_ndcg(self, queries_list: np.ndarray,\n","                        true_labels: torch.FloatTensor, preds: torch.FloatTensor) -> float:\n","       \n","        score = []\n","        for id in np.unique(queries_list):\n","            idxs = queries_list == id\n","            score.append(ndcg(true_labels[idxs], preds[idxs], gain_scheme=\"exp2\", k=15))\n","        return np.array(score).mean()\n","\n","    def fit(self):\n","        # Устанавливаем seed для воспроизводимости результатов\n","        set_seed(0)\n","\n","        # Создаем массивы для предсказанных значений для обучающей и тестовой выборок\n","        predicted_train = torch.zeros_like(self.ys_train)\n","        predicted_test = torch.zeros_like(self.ys_test)\n","\n","        # Обучаем n_estimators деревьев\n","        for k in tqdm(range(self.n_estimators)):\n","            # Обучаем одно дерево и сохраняем его\n","            tree, feature_indices = self._train_one_tree(k, predicted_train)\n","            self.trees.append(tree)\n","            self.idxs_array.append(feature_indices)\n","\n","            # Применяем обученное дерево к обучающей и тестовой выборкам\n","            prediction_train = tree.predict(self.X_train[:, feature_indices])\n","            prediction_test = tree.predict(self.X_test[:, feature_indices])\n","\n","            # Обновляем предсказанные значения для обучающей и тестовой выборок\n","            predicted_train -= self.lr * prediction_train\n","            predicted_test -= self.lr * prediction_test\n","\n","            # Вычисляем NDCG на тестовой выборке и сохраняем его\n","            ndcg = self._calc_data_ndcg(self.query_ids_test, self.ys_test, predicted_test)\n","            self.all_ndcg.append(ndcg)\n","\n","            # Если текущее значение NDCG лучше, чем лучшее значение, сохраняем его\n","            if ndcg > self.best_ndcg:\n","                self.best_ndcg = ndcg\n","    \n","        # Выбираем лучшее дерево и удаляем оставшиеся\n","        last = self.all_ndcg.index(self.best_ndcg)\n","        self.trees = self.trees[0:last+1]\n","        self.idxs_array = self.idxs_array[0:last+1]\n","\n","        # Выводим лучший результат NDCG\n","        print(f'Total NDCG score {self.best_ndcg}')\n","\n","    def predict(self, data: torch.FloatTensor) -> torch.FloatTensor:\n","        # Выбираем признаки, используемые каждым деревом\n","        feature_indices = torch.tensor(self.idxs_array, dtype=torch.long)\n","        data_subset = torch.index_select(data, dim=1, index=feature_indices)\n","\n","        # Получаем предсказания на всех деревьях\n","        tree_preds = torch.stack([dt.predict(data_subset) for dt in self.trees])\n","\n","        # Вычисляем сумму предсказаний всех деревьев и умножаем на learning rate\n","        ans = -self.lr * torch.sum(tree_preds, dim=0)\n","\n","        return ans\n","\n","    def _compute_lambdas(self, y_true: torch.FloatTensor, y_pred: torch.FloatTensor) -> torch.FloatTensor:\n","        ndcg_scheme = \"exp2\"\n","        ideal_dcg = dcg(y_true, y_true, ndcg_scheme)\n","        N = 0\n","        if ideal_dcg != 0:\n","            N = 1 / ideal_dcg\n","            \n","        y_true_temp = y_true.reshape(-1, 1)\n","        y_pred_temp = y_pred.reshape(-1, 1)\n","        \n","        _, rank_order = torch.sort(y_true_temp, descending=True, axis=0)\n","        rank_order += 1\n","        with torch.no_grad():\n","            # получаем все попарные разницы скоров в батче\n","            pos_pairs_score_diff = 1.0 + torch.exp((y_pred_temp - y_pred_temp.t()))\n","\n","            # поставим разметку для пар, 1 если первый документ релевантнее\n","            # -1 если второй документ релевантнее\n","            Sij = self._compute_labels_in_batch(y_true_temp)\n","            # посчитаем изменение gain из-за перестановок\n","            gain_diff = self._compute_gain_diff(y_true_temp, ndcg_scheme)\n","\n","            # посчитаем изменение знаменателей-дискаунтеров\n","            decay_diff = (1.0 / torch.log2(rank_order + 1.0)) - (1.0 / torch.log2(rank_order.t() + 1.0))\n","            # посчитаем непосредственное изменение nDCG\n","            delta_ndcg = torch.abs(N * gain_diff * decay_diff)\n","            # посчитаем лямбды\n","            lambda_update =  (0.5 * (1 - Sij) - 1 / pos_pairs_score_diff) * delta_ndcg\n","            lambda_update = torch.sum(lambda_update, dim=1, keepdim=True)\n","\n","            return lambda_update\n","    \n","    def _compute_labels_in_batch(self, y_true: torch.FloatTensor):\n","        # разница релевантностей каждого с каждым объектом\n","        rel_diff = y_true - y_true.t()\n","\n","        # 1 в этой матрице - объект более релевантен\n","        pos_pairs = (rel_diff > 0).type(torch.float32)\n","\n","        # 1 тут - объект менее релевантен\n","        neg_pairs = (rel_diff < 0).type(torch.float32)\n","        Sij = pos_pairs - neg_pairs\n","        return Sij\n","\n","\n","    def _compute_gain_diff(self, y_true: torch.FloatTensor, gain_scheme: str):\n","        if gain_scheme == \"exp2\":\n","            gain_diff = torch.pow(2.0, y_true) - torch.pow(2.0, y_true.t())\n","        elif gain_scheme == \"diff\":\n","            gain_diff = y_true - y_true.t()\n","        else:\n","            raise ValueError(f\"{gain_scheme} method not supported\")\n","        return gain_diff\n","    \n","    def _ndcg_k(self, ys_true, ys_pred, ndcg_top_k) -> float:\n","        try:\n","            return ndcg(ys_true, ys_pred, gain_scheme='exp2', k=ndcg_top_k)\n","        except ZeroDivisionError:\n","            return float(0)\n","\n","    def save_model(self, path: str):\n","        pickle.dump(self, open('%s.lmart' % path, \"wb\"), protocol=2)\n","                        \n","    def load_model(self, path: str):\n","        model = pickle.load(open(path, \"rb\"))\n","        self.X_train = model.X_train\n","        self.ys_train = model.ys_train\n","        self.X_test = model.X_test\n","        self.ys_test = model.ys_test\n","        self.ndcg_top_k = model.ndcg_top_k\n","        self.n_estimators = model.n_estimators \n","        self.lr = model.lr\n","        self.max_depth = model.max_depth\n","        self.min_samples_leaf = model.min_samples_leaf \n","        self.subsample = model.subsample\n","        self.colsample_bytree = model.colsample_bytree\n","        self.trees = model.trees\n","        self.idxs_array = model.idxs_array\n","        self.all_ndcg = model.all_ndcg\n","        self.best_ndcg = model.best_ndcg"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}